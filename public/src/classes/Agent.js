import Environment from './Environment';
import policy_x from './QLearning_Policy_x';
import policy_o from './Qlearning_Policy_o';

/**
 * @desc The following class describes the Agent
 * @param {Boolean} starting indicates whether the agent is the starting player or not.
 * @param {Number} max_depth indicates the maximum depth the Agent should recurse for in Minimax algorithm
 * @param {Boolean} use_minimax determines whether to use Minimax or Q-Learning for the Agent program's policy.
 */
class Agent {
	
	constructor(starting, max_depth = -1, use_minimax=true) {
        this.max_depth = max_depth;
		this.nodes_map = new Map();
		this.starting = starting;
		this.use_minimax = use_minimax;
    }

	/**
	 * @desc This function implements the Minimax algorithm to decide the best action to take
	 * @param {Object} percept_sequence Percept obtained by the sensors of the Agent from the Environment (The "Board") 
	 * @param {Boolean} maximizing indicates the turn for the move i.e maximizing or minimizing
	 * @param {Number} depth inicates the depth of minimax tree the recursive function is in. 
	 * @return {Number} the index of the next best move
	 */
	minimaxLookAhead(percept_sequence, maximizing, depth) {
		//clear nodes_map if the function is called for a new move
		if(depth == 0) this.nodes_map.clear();
		//If the board state is a terminal one, return the heuristic value
		if(percept_sequence.isTerminal || depth == this.max_depth ) {
			if(percept_sequence.winner == 'x') {
				return 100 - depth;
			} else if (percept_sequence.winner == 'o') {
				return -100 + depth;
			} 
			return 0;
		}

		//Initialize best to the lowest possible value for maximizing turn and highest possible value for minimizing turn
		let best = maximizing? -100: 100;
		//Loop through all empty cells
		percept_sequence.moves.forEach(index => {
			//Initialize a new board with the current state (slice() is used to create a new array and not modify the original)
			let child = new Environment(percept_sequence.state.slice());
			//Create a child node by inserting the maximizing/minimizing symbol into the current empty cell
			child.performAction({position: index, symbol: maximizing? 'x' : 'o'});

			//Recursively calling minimaxLookAhead this time with the new board and next turn and incrementing the depth
			let node_value = this.minimaxLookAhead(child.getPercept(), !maximizing, depth + 1);
			//Updating best value
			best = maximizing? Math.max(best, node_value): Math.min(best, node_value);
			
			//If it's the main function call, not a recursive one, map each heuristic value with it's moves indicies
			if(depth == 0) {
				//Comma seperated indicies if multiple moves have the same heuristic value
				let moves = this.nodes_map.has(node_value) ? `${this.nodes_map.get(node_value)},${index}` : index;
				this.nodes_map.set(node_value, moves);
			}
		});
		//If it's the main call, return the index of the best move or a random index if multiple indicies have the same value
		if(depth == 0) {
			if(typeof this.nodes_map.get(best) == 'string') {
				let arr = this.nodes_map.get(best).split(',');
				let rand = Math.floor(Math.random() * arr.length);
				var ret = arr[rand];
			} else {
				ret = this.nodes_map.get(best);
			}
			//run a callback after calculation and return the index
			return ret;
		}
		//If not main call (recursive) return the heuristic value for next calculation
		return best;
	}

	/**
	 * @desc This function uses the QTable(s) generated by QLearner.js (Node.js program) to decide the best action to take
	 * @param {Object} percept_sequence Percept obtained by the sensors of the Agent from the Environment (The "Board")
	 * @param {Boolean} starting indicates the turn for the move i.e maximizing or minimizing
	 * @return {Number} the index of the next best move
	 */
	qtableLookAhead(percept_sequence, starting) {
		let value_max = -999;
		let symbol = 'x';
		let policy = policy_x;
		let best_action = -1;
		//checking if Agent is not the starting player
		if (!starting) {
			symbol ='o';
			policy = policy_o;
		}
		// iterating over every empty cell
		percept_sequence.moves.forEach((cell, index) => {
			//Initialize a new board state with the current state (slice() is used to create a new array and not modify the original)
			let next_state = percept_sequence.state.slice();
			//insert the agent symbol
			next_state[cell] = symbol;
			//generate a string form of current state
			let next_state_hash = next_state.toString();
			//check if new state is present in the Q-table else assign 0 to value
			let value = (policy[next_state_hash] !== undefined)? policy[next_state_hash] : 0;
			// if value is greater than the greatest value so far then we found a new best action
                if (value > value_max) {
                    value_max = value;
                    best_action = cell;
                }
		});
		//return the best action(position to take)
		return best_action;
	}

	/**
	 * @desc Method implementing the sensor of the Agent
	 * @param {Object} Environment object describing the Environment of the Agent
	 * @return {Object} returns percept receivedd from the Environment
	 */
	sensor(Environment) {
		return Environment.getPercept();
	}

	/**
	 * @desc agent function maps the percept to an appropriate action. It is dictated by the agent program which has two algorithms - Minimax and Q-Learning
	 * @param {Object} percept_sequence object representing the percept_sequence received from the sensors 
	 * @return {Number} returns the next best action(position) to be passed on to the actuator.
	 */
	agentFunction(percept_sequence) {
		let best_position = 0;
		if (this.use_minimax) {
			best_position = this.minimaxLookAhead(percept_sequence, this.starting, 0);
		}
		else {
			best_position = this.qtableLookAhead(percept_sequence, this.starting);
		}
		return best_position;
	}

	/**
	 * @desc method implementing actuator of the Agent to perform action on the Environment
	 * @param {Object} Environment object describing the Environment of the Agent
	 * @param {Number} best_position best position to place the symbol
	 * @return {Boolean} if actuator successfully performs action then returns true else returns false 
	 */
	actuator(Environment, best_position) {
		const action = {position: best_position, symbol: this.starting? 'x' : 'o'};
		return Environment.performAction(action);
	}
	/**
	 * @desc If computer is going to start, choose a random cell as long as it is the center or a corner
	 * @param {Object} Environment object describing the Environment of the Agent
	 * @return {Number} first random action the Agent takes
	 */
	startingMove(Environment) {
		let center_and_corners = [0,2,4,6,8];
        let first_choice = center_and_corners[Math.floor(Math.random()*center_and_corners.length)];
		this.actuator(Environment, first_choice);
		return first_choice;
	}
}

export default Agent;